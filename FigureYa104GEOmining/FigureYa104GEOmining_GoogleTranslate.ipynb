{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from Bio import Entrez\n",
    "from metapub import PubMedFetcher\n",
    "from pytablewriter import MarkdownTableWriter\n",
    "from tqdm import tqdm\n",
    "from googletrans import Translator  # 导入 googletrans\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FigureYa104GEOmining\n",
    "\n",
    "title: \"FigureYa104GEOmining\"\n",
    "author: \"Yu Sun, Xuan Da, Taojun Ye\"\n",
    "reviewer: \"Ying Ge\"\n",
    "date: \"2025-5-20\"\n",
    "output: html_document\n",
    "\n",
    "## 需求描述\n",
    "\n",
    "在GEO数据库中检索到的高通量数据，想批量获得它们出自哪篇文章，标注影响因子，输出文本文件和网页，网页文件里有链接到每篇文章的pubmed页面。\n",
    "\n",
    "**用法参考这篇帖子：**<https://mp.weixin.qq.com/s/G-CQhNEJBmMRuDe2kxND_w>\n",
    "\n",
    "##Requirement description: \n",
    "\n",
    "Retrieve high-throughput data from GEO database, want to obtain in bulk which article they come from, annotate impact factors, output text files and web pages, with links to the pubmed pages of each article in the web file. \n",
    "\n",
    "**Refer to this post for usage:**< https://mp.weixin.qq.com/s/G-CQhNEJBmMRuDe2kxND_w >\n",
    "\n",
    "## 应用场景\n",
    "\n",
    "场景一：老板让测序，怎样设计实验呢？参考测同样数据的文章是怎样设计实验的。\n",
    "\n",
    "场景二：测序数据回来了，怎样分析？能画哪些图？结果怎样描述？参考类似的数据的文章吧！\n",
    "\n",
    "场景三：想结合已发表的数据做整合分析，哪套数据更靠谱？先看影响因子高的文章里的数据吧！\n",
    "\n",
    "##Application scenario \n",
    "\n",
    "Scenario 1: How to design experiments when the boss asks sequencing? How to design experiments based on articles with the same data for reference testing. \n",
    "\n",
    "Scenario 2: The sequencing data is back, how to analyze it? What pictures can be drawn? How to describe the results? Refer to articles with similar data! \n",
    "\n",
    "Scenario 3: Do you want to integrate and analyze published data? Which set of data is more reliable? Let's first look at the data in articles with high impact factors!\n",
    "\n",
    "## 环境设置\n",
    "\n",
    "下载并安装Anaconda发行版，https://www.anaconda.com/distribution/#download-section\n",
    "\n",
    "里面已经包含了运行本文档所需的Python3、ipython、Jupyter notebook。\n",
    "\n",
    "需要额外安装BioPython、metapub和pytablewriter，由于eutils在比较新的版本中更新了API导致不能向前兼容，所以也需要重新安装一个比较旧的版本，在终端运行以下命令来安装：\n",
    "\n",
    "##Environment settings\n",
    "Download and install the Anaconda distribution, https://www.anaconda.com/distribution/#download -\n",
    "\n",
    "The section already includes the Python 3, iPython, and Jupyter notebooks required to run this document. \n",
    "\n",
    "Additional installations of BioPython, metapub, and pytablewriter are required. As eutils has updated its API in a newer version, it cannot be forward compatible. Therefore, an older version needs to be reinstalled by running the following command on the terminal:\n",
    "\n",
    "```bash\n",
    "conda install -c anaconda biopython\n",
    "pip install metapub\n",
    "pip install eutils==0.5.0\n",
    "pip install pytablewriter\n",
    "pip install markdown\n",
    "pip install tqdm # Anaconda自带tqdm库\n",
    "```\n",
    "\n",
    "## 输入\n",
    "\n",
    "把要检索的关键词写进下面代码区的”term = “的后面，例如`(gds pubmed[Filter]) AND \"Drosophila melanogaster\"[orgn:__txid7227] AND ATAC-seq`。\n",
    "\n",
    "建议先在<https://www.ncbi.nlm.nih.gov/geo/>网站上检索，尝试好合适的关键词后，再来提取文献。\n",
    "\n",
    "开头加上`(gds pubmed[Filter])` ，就会过滤掉那些还没有发表文章的数据。\n",
    "\n",
    "##Input\n",
    "Write the keywords to be searched after \"term=\" in the code area below, for example, ` (gds pubmed [Filter]) AND \"Drosophila melanogas\" [ergn: __txid7227] AND ATAC seq `. \n",
    "Suggest starting with< https://www.ncbi.nlm.nih.gov/geo/ >Search on the website, try appropriate keywords, and then extract literature. \n",
    "Adding '(gds pubmed [Filter]' at the beginning will filter out data that has not yet been published.\n",
    "\n",
    "## 运行代码\n",
    "\n",
    "打开Anaconda——Jupyter Notebook，打开本文档，在Jupyter Notebook中点击Run按钮，运行下面的代码。\n",
    "\n",
    "**加速：**建议自己注册一个NCBI的账号，然后点击右上角自己的邮箱，申请API key，以加快检索速度：E-utils users are allowed 3 requests/second without an API key. Create an API key to increase your e-utils limit to 10 requests/second。\n",
    "\n",
    "把你的API key添加到代码区的“Entrez.api_key = ”后面\n",
    "\n",
    "API key的获取方法详见<https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/>\n",
    "\n",
    "##Run code\n",
    "Open Anaconda - Jupyter Notebook, open this document, click the Run button in Jupyter Notebook, and run the following code. \n",
    "\n",
    "**Acceleration: * * It is recommended to register an NCBI account and click on your email in the upper right corner to apply for an API key to speed up the retrieval process E-utils users are allowed 3 requests/second without an API key. Create an API key to increase your e-utils limit to 10 requests/second。 \n",
    "\n",
    "Add your API key to the \"Entrez. api_key=\" section in the code area. \n",
    "\n",
    "For details on how to obtain the API key, please refer to< https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ >\n",
    "\n",
    "## 下面是代码区\n",
    "\n",
    "##Below is the code area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索 '(gds pubmed[Filter]) AND \"myocardial infarction\" AND \"Homo sapiens\"[orgn:__txid9606]' 返回了 435 条结果。正在解析...\n",
      "Search for '(gds pubmed[Filter]) AND \"myocardial infarction\" AND \"Homo sapiens\"[orgn:__txid9606]' returned 435 results. Parsing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████                                    | 63/435 [00:12<01:11,  5.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from Bio import Entrez\n",
    "from metapub import PubMedFetcher\n",
    "from pytablewriter import MarkdownTableWriter\n",
    "from tqdm import tqdm\n",
    "from googletrans import Translator  \n",
    "from retry import retry\n",
    "\n",
    "# 读取期刊影响因子的CSV文件\n",
    "# Read the CSV file containing journal impact factors\n",
    "df = pd.read_csv('impact_factor.csv')\n",
    "\n",
    "# 获取期刊的影响因子\n",
    "# Retrieve the impact factor of a journal\n",
    "def get_impact_factor(issn_eissn):\n",
    "    # 使用ISSN或E-ISSN查找期刊的影响因子\n",
    "    # Use ISSN or E-ISSN to look up the journal's impact factor\n",
    "    impact_factor = df.loc[(df['ISSN'] == issn_eissn) | (df['E-ISSN'] == issn_eissn), 'Impact Factor'].values\n",
    "    if len(impact_factor) > 0:\n",
    "        return impact_factor[0]\n",
    "    else:\n",
    "        return '-NA-'\n",
    "\n",
    "# 设置Entrez模块的邮箱和API key（用于NCBI数据库访问）\n",
    "# Set up email and API key for Entrez module (for NCBI database access)\n",
    "Entrez.email = \"yetaojun0709@gmail.com\"\n",
    "Entrez.api_key = \"e479d557c9c4a6533a56188d731704f66107\"\n",
    "\n",
    "# 设置PubMed搜索条件：GDS数据库中关于人类心肌梗死的研究\n",
    "# Set PubMed search criteria: Studies on myocardial infarction in humans from GDS database\n",
    "term = '(gds pubmed[Filter]) AND \"myocardial infarction\" AND \"Homo sapiens\"[orgn:__txid9606]'\n",
    "# 搜索并获取符合条件的文章ID列表\n",
    "# Search and retrieve the list of article IDs matching the criteria\n",
    "handle = Entrez.esearch(db=\"gds\", term=term, retmax=100000)\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "# 创建PubMed文章获取器实例\n",
    "# Create an instance of PubMedFetcher to retrieve articles\n",
    "fetch = PubMedFetcher()\n",
    "\n",
    "# 设置Markdown表格的标题行（中英文标题、年份、影响因子等）\n",
    "# Set headers for the Markdown table (Title, Chinese Title, Year, Impact Factor, etc.)\n",
    "writer = MarkdownTableWriter()\n",
    "# 添加中文标题一栏\n",
    "# Add a column for Chinese title\n",
    "writer.headers = [\"Title\", \"Title (Chinese)\", \"Year\", \"Impact Factor\", \"Journal\", \"PMID\", \"GEO accession\", \"Authors\"]\n",
    "writer.value_matrix = []\n",
    "\n",
    "# 定义一个带有重试功能的翻译函数，用于翻译文章标题\n",
    "# Define a translation function with retry capability for article titles\n",
    "@retry(tries=3, delay=2)  # 最多尝试3次，每次尝试之间间隔2秒\n",
    "# Retry up to 3 times with a 2-second delay between attempts\n",
    "def translate_title(title):\n",
    "    return translator.translate(title, dest='zh-CN').text\n",
    "\n",
    "# 初始化Google翻译器\n",
    "# Initialize Google Translator\n",
    "translator = Translator()\n",
    "\n",
    "# 打印搜索结果信息\n",
    "# Print search result statistics\n",
    "print(\"搜索 '{}' 返回了 {} 条结果。正在解析...\".format(term, len(record['IdList'])))\n",
    "print(\"Search for '{}' returned {} results. Parsing...\".format(term, len(record['IdList'])))\n",
    "\n",
    "# 处理单个GDS ID并提取相关文章信息\n",
    "# Process a single GDS ID and extract relevant article information\n",
    "def process_gds_id(gds_id):\n",
    "    result = []\n",
    "    gds_url = f'https://www.ncbi.nlm.nih.gov/gds/?term={gds_id}'\n",
    "    try:\n",
    "        # 打开NCBI GDS记录页面\n",
    "        # Open the NCBI GDS record page\n",
    "        soup = BeautifulSoup(urlopen(gds_url), 'lxml')\n",
    "    except Exception as e:\n",
    "        # print(\"访问NCBI API出错：\", e)\n",
    "        # print(\"Error accessing NCBI API:\", e)\n",
    "        return result\n",
    "\n",
    "    # 获取GEO accession号码和对应的URL\n",
    "    # Extract GEO accession number and its corresponding URL\n",
    "    accn = soup.body.form('dl', class_=\"rprtid\")[0].contents[1].string\n",
    "    accn_url = f'https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={accn}'\n",
    "    try:\n",
    "        # 打开GEO accession页面\n",
    "        # Open the GEO accession page\n",
    "        soup = BeautifulSoup(urlopen(accn_url), 'lxml')\n",
    "    except:\n",
    "        # print(\"访问NCBI API出错，请等待或更换API key。\")\n",
    "        # print(\"Error accessing NCBI API. Please wait or change API key.\")\n",
    "        return result\n",
    "    \n",
    "    # 获取文章的PubMed ID\n",
    "    # Extract PubMed ID of the article\n",
    "    pubmed_id_element = soup.body.find('span', class_=\"pubmed_id\")\n",
    "    if pubmed_id_element:\n",
    "        pmids = pubmed_id_element['id'].split(',')\n",
    "        # 处理每个PubMed ID\n",
    "        # Process each PubMed ID\n",
    "        for pmid in pmids:\n",
    "            pmid_url = f'https://www.ncbi.nlm.nih.gov/pubmed/{pmid}'\n",
    "            try:\n",
    "                # 通过PubMed ID获取文章信息\n",
    "                # Retrieve article information using PubMed ID\n",
    "                article = fetch.article_by_pmid(pmid)\n",
    "            except:\n",
    "                # print(\"访问NCBI API出错，请等待或更换API key。\")\n",
    "                # print(\"Error accessing NCBI API. Please wait or change API key.\")\n",
    "                return result\n",
    "\n",
    "            # 获取文章的ISSN或E-ISSN号码\n",
    "            # Get the ISSN or E-ISSN of the journal\n",
    "            issn_eissn = article.issn or article.e_issn or '-NA-'\n",
    "            # 获取文章所属期刊的影响因子\n",
    "            # Retrieve the journal's impact factor\n",
    "            if_value = get_impact_factor(issn_eissn)\n",
    "            # 格式化作者列表（前n-1个作者用逗号连接，最后一个用and连接）\n",
    "            # Format author list (connect first n-1 authors with commas, last with \"and\")\n",
    "            author_list = \", \".join(article.authors[:-1]) + \" and \" + article.authors[-1]\n",
    "\n",
    "            # 翻译文章标题\n",
    "            # Translate article title\n",
    "            try:\n",
    "                translated_title = translate_title(article.title)\n",
    "            except Exception as e:\n",
    "                print(f\"翻译标题时出现错误：{e}\")\n",
    "                print(f\"Error translating title: {e}\")\n",
    "                translated_title = \"翻译失败\"\n",
    "                translated_title = \"Translation Failed\"\n",
    "            result.append([article.title, translated_title, article.year, if_value, article.journal, '[' + pmid + '](' + pmid_url + ')', '[' + accn + '](' + accn_url + ')', author_list])\n",
    "    return result\n",
    "        \n",
    "# 设置最大线程数（通常为CPU核心数的2倍）\n",
    "# Set maximum number of threads (typically twice the number of CPU cores)\n",
    "max_workers = 2 * os.cpu_count()\n",
    "# 使用ThreadPoolExecutor并行处理所有GDS ID，提高处理效率\n",
    "# Use ThreadPoolExecutor to process all GDS IDs in parallel for efficiency\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    results = list(tqdm(executor.map(process_gds_id, record['IdList']), total=len(record['IdList'])))\n",
    "# 将所有处理结果添加到Markdown表格中\n",
    "# Add all processed results to the Markdown table\n",
    "for result in results:\n",
    "    writer.value_matrix.extend(result)\n",
    "# 将结果写入文件并打印内容\n",
    "# Write results to file and print content\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出\n",
    "\n",
    "在当前文件夹里生成分两个文件：\n",
    "\n",
    "1. `GEO_citations.txt`：文本文件，包含文章信息的汇总\n",
    "\n",
    "2. `GEO_citations.html`：网页中嵌入的三线表，蓝色字带链接，点击GSE ID可直达数据的GEO页面，点击PMID可直达文章的Pubmed页面。默认为按照相关性排序“Sort by Default order”，可以复制到Excel中自行排序、筛选等操作；链接也会保留到Excel文件中，点击链接可直接跳转至paper网页。\n",
    "\n",
    "## output\n",
    "\n",
    "Generate two files in the current folder:\n",
    "\n",
    "1. GEO_citations. txt: Text file containing a summary of article information\n",
    "\n",
    "2. GEO_citations. html: A three line table embedded in the webpage, with blue text and links. Clicking on GSE ID will take you directly to the GEO page of the data, and clicking on PMID will take you directly to the Pubmed page of the article. The default sorting is \"Sort by Default order\" based on relevance, which can be copied to Excel for sorting, filtering, and other operations; The link will also be retained in the Excel file, and clicking on the link will directly redirect to the paper webpage.\n",
    "\n",
    "## 特殊情况的说明\n",
    "\n",
    "1. 如果遇到TimeoutError，换个网络好的地方再试。\n",
    "\n",
    "2. 偶尔会遇到影响因子那里都是NA的情况，可能是<https://www.scijournal.org>网站访问不畅，稍后重试即可。\n",
    "\n",
    "3. 个别期刊在这个网站上检索不到影响因子，例如PNAS、NAR，所以会显示为NA。\n",
    "\n",
    "4. 如果文章题目中出现特殊符号，例如“<”，题目会在此断掉，这是Entrez包的一个bug。\n",
    "\n",
    "##Explanation of special circumstances\n",
    "1. If encountering TimeoutError, try again in a better network location.\n",
    "2. Occasionally encountering situations where the influencing factors are all NA, it may be< https://www.scijournal.org >The website is not accessible, please try again later.\n",
    "3. Some journals cannot retrieve impact factors on this website, such as PNAS and NAR, so they will be displayed as NA.\n",
    "4. If there is a special symbol in the title of the article, such as \"<\", the title will be broken here, which is a bug in the Entrez package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "print(IPython.sys_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fresh_env)",
   "language": "python",
   "name": "fresh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
